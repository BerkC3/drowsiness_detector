# Real-Time Driver Drowsiness Detection

A real-time drowsiness and yawn detection system using a webcam. Combines **dlib facial landmarks** with a lightweight **CNN** (PyTorch) for eye-state classification, and uses the **Mouth Aspect Ratio (MAR)** for yawn detection.

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.x-red)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green)

## How It Works

1. **Face detection** — dlib's HOG-based frontal face detector finds the largest face in frame.
2. **Landmark prediction** — 68-point shape predictor locates eyes, mouth, and key facial points.
3. **Eye-state (CNN)** — Square crops around each eye are preprocessed (CLAHE + normalize) and fed into a small CNN that outputs open/closed confidence. If both eyes stay closed for enough consecutive frames, a **drowsiness alert** triggers.
4. **Yawn detection (MAR)** — Inner mouth landmarks compute the Mouth Aspect Ratio. Sustained high MAR triggers a **yawning alert**.
5. **Head pose estimation** — 6 facial landmarks are matched against a 3D face model via `cv2.solvePnP` to extract pitch/yaw/roll angles. If the head drops forward (negative pitch) for too long, a **head drop alert** triggers. A nose direction line is drawn for visual feedback.

## Project Structure

```
drowsiness_detector/
├── main.py              # entry point
├── config.py            # all tuneable parameters
├── face_processor.py    # dlib detection + CNN inference
├── driver_monitor.py    # main loop, ROI extraction, state tracking, drawing
├── model.py             # CNN architecture (EyeStateModel)
├── train_model.py       # training script (GPU-accelerated)
├── debug_eyes.py        # visual debugger for eye ROIs
├── requirements.txt
├── drowsiness_model.pth # trained weights (generated by train_model.py)
├── shape_predictor_68_face_landmarks.dat  # dlib model (download separately)
└── dataset/             # training data (not included)
    ├── Closed_Eyes/
    └── Open_Eyes/
```

## Requirements

- Python 3.10+
- Webcam
- NVIDIA GPU recommended for training (CPU works but slower)

## Setup

### 1. Install dependencies

**With GPU (CUDA 12.4):**
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu124
pip install -r requirements.txt
```

**CPU only:**
```bash
pip install -r requirements.txt
```

### 2. Download the dlib shape predictor

Download and extract `shape_predictor_68_face_landmarks.dat` into the project root:

```bash
# Linux/macOS
wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
bzip2 -d shape_predictor_68_face_landmarks.dat.bz2
```

On Windows, download manually from http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 and extract with 7-Zip.

### 3. Prepare the dataset

Place your eye images in:
```
dataset/
├── Closed_Eyes/   (class 0)
└── Open_Eyes/     (class 1)
```

The [MRL Eye Dataset](http://mrl.cs.vsb.cz/eyedataset) works well for this.

### 4. Train the model

```bash
python train_model.py
```

Training preloads all images into RAM and runs augmentation on-the-fly. On an RTX 4080 Super, each epoch takes a few seconds.

Output: `drowsiness_model.pth`

### 5. Run

```bash
python main.py
```

Press **q** to quit.

## Configuration

All parameters live in `config.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `EYE_CONFIDENCE_THRESHOLD` | 0.5 | CNN probability below this = "closed" |
| `CONSEC_FRAMES` | 48 | Frames both eyes must be closed to trigger alert |
| `MAR_THRESHOLD` | 0.45 | MAR above this = "yawning" |
| `YAWN_CONSEC_FRAMES` | 20 | Frames mouth must be open to trigger alert |
| `HEAD_PITCH_THRESHOLD` | -15.0 | Pitch below this (degrees) = head dropping |
| `HEAD_DROP_CONSEC_FRAMES` | 30 | Frames head must be dropped to trigger alert |
| `CAMERA_INDEX` | 0 | Webcam index |

## Debugging

```bash
python debug_eyes.py
```

Opens separate windows showing raw eye ROIs, preprocessed 24x24 crops, and model confidence in real-time. Press **s** to save a snapshot.

## Notes

- The CNN uses CLAHE preprocessing to handle varying lighting and reduce glare from glasses.
- Eye ROIs are cropped as squares (1.8x eye width) to keep a consistent aspect ratio regardless of eye state.
- Head pose uses `cv2.solvePnP` with 6 key landmarks (nose, chin, eye corners, mouth corners) mapped to a generic 3D face model. A blue nose direction line shows where the head is pointing.
- Inference runs on CPU by design — for a single 24x24 image, CPU is faster than GPU due to transfer overhead.

## License

MIT
